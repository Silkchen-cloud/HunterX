# 使用指南

本文档详细介绍如何使用招聘Agent进行候选人搜索和管理。

## 基本概念

### Agent工作流程
1. **数据抓取**: 从LinkedIn、Facebook、X等平台搜索候选人
2. **数据处理**: 清洗、标准化和结构化候选人信息
3. **数据存储**: 将处理后的数据保存到Notion数据库
4. **定期更新**: 通过调度器定期执行上述流程

### 数据来源
- **LinkedIn**: 专业社交网络，包含详细的职业信息
- **Facebook**: 社交网络，主要获取基本信息和工作经历
- **X (Twitter)**: 微博平台，了解候选人的专业观点和活跃度

## 命令行使用

### 基本命令

#### 1. 运行Agent
```bash
# 使用默认配置运行一次
python main.py run

# 指定搜索关键词
python main.py run --keywords "python developer,data scientist"

# 指定搜索地点
python main.py run --locations "Beijing,Shanghai,Guangzhou"

# 限制结果数量
python main.py run --max-results 20

# 只搜索特定平台
python main.py run --platform linkedin
python main.py run --platform facebook
python main.py run --platform twitter
```

#### 2. 查看状态
```bash
python main.py status
```

输出示例：
```
📊 Agent状态:
  配置文件: ✅ 已加载
  Notion连接: ✅ 正常
  LinkedIn爬虫: ✅ 可用
  Facebook爬虫: ❌ 不可用
  Twitter爬虫: ✅ 可用

📈 数据统计:
  总候选人数: 156
  LinkedIn: 89
  Facebook: 23
  Twitter: 44
  最后更新: 2025-07-27T10:30:00
```

#### 3. 搜索现有候选人
```bash
# 按姓名搜索
python main.py search "张三"

# 按公司搜索
python main.py search "阿里巴巴"

# 按职位搜索
python main.py search "软件工程师"
```

#### 4. 设置定时任务
```bash
# 每天上午9点运行
python main.py schedule --cron "0 9 * * *"

# 每2小时运行一次
python main.py schedule --interval 120

# 使用高级调度器
python main.py schedule --use-apscheduler --cron "0 9 * * 1-5"
```

#### 5. 配置管理
```bash
# 启动配置向导
python main.py config
```

### 高级用法

#### 组合参数使用
```bash
# 搜索北京的Python开发者，最多20个结果，只在LinkedIn搜索
python main.py run \
  --keywords "python developer" \
  --locations "Beijing" \
  --max-results 20 \
  --platform linkedin
```

#### 批量操作
```bash
# 创建批量搜索脚本
cat > batch_search.sh << 'EOF'
#!/bin/bash
python main.py run --keywords "frontend developer" --platform linkedin
python main.py run --keywords "backend developer" --platform linkedin
python main.py run --keywords "data scientist" --platform twitter
EOF

chmod +x batch_search.sh
./batch_search.sh
```

## 配置详解

### 搜索配置

#### 关键词设置
```env
# 多个关键词用逗号分隔
SEARCH_KEYWORDS=software engineer,data scientist,product manager,frontend developer

# 支持中英文混合
SEARCH_KEYWORDS=软件工程师,数据科学家,python developer,AI engineer
```

#### 地点设置
```env
# 支持城市名称
SEARCH_LOCATIONS=Beijing,Shanghai,Shenzhen,Hangzhou

# 支持英文城市名
SEARCH_LOCATIONS=Beijing,Shanghai,New York,San Francisco

# 支持省份/州
SEARCH_LOCATIONS=California,Guangdong,Beijing
```

#### 结果数量控制
```env
# 每个平台的最大结果数
MAX_RESULTS_PER_PLATFORM=50

# 请求间隔 (秒)，避免被限制
REQUEST_DELAY=1.5
```

### 平台特定配置

#### LinkedIn配置
```env
# 方式1: 使用登录凭据 (不推荐)
LINKEDIN_USERNAME=your_email@example.com
LINKEDIN_PASSWORD=your_password

# 方式2: 使用第三方服务 (推荐)
# 配置第三方API密钥
LINKEDIN_API_KEY=your_third_party_api_key
```

#### Twitter配置
```env
# 方式1: 使用Bearer Token (推荐)
TWITTER_BEARER_TOKEN=your_bearer_token

# 方式2: 使用OAuth 1.0a
TWITTER_API_KEY=your_api_key
TWITTER_API_SECRET=your_api_secret
TWITTER_ACCESS_TOKEN=your_access_token
TWITTER_ACCESS_TOKEN_SECRET=your_access_token_secret
```

#### Facebook配置
```env
# 使用Graph API访问令牌
FACEBOOK_ACCESS_TOKEN=your_access_token

# 可选：指定API版本
FACEBOOK_API_VERSION=v18.0
```

### 日志配置
```env
# 日志级别
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR

# 日志文件
LOG_FILE=logs/recruitment_agent.log

# 日志格式 (在logger.py中配置)
```

## 定时任务详解

### Cron表达式
```bash
# 格式: 秒 分 时 日 月 周
# 每天上午9点
python main.py schedule --cron "0 9 * * *"

# 工作日上午9点
python main.py schedule --cron "0 9 * * 1-5"

# 每周一上午9点
python main.py schedule --cron "0 9 * * 1"

# 每2小时
python main.py schedule --cron "0 */2 * * *"

# 每天上午9点和下午5点
python main.py schedule --cron "0 9,17 * * *"
```

### 间隔调度
```bash
# 每30分钟
python main.py schedule --interval 30

# 每2小时
python main.py schedule --interval 120

# 每天 (1440分钟)
python main.py schedule --interval 1440
```

### 系统服务部署

#### 创建systemd服务 (Linux)
```bash
# 创建服务文件
sudo tee /etc/systemd/system/recruitment-agent.service << 'EOF'
[Unit]
Description=Recruitment Agent
After=network.target

[Service]
Type=simple
User=ubuntu
WorkingDirectory=/path/to/recruitment-agent
Environment=PATH=/path/to/recruitment-agent/venv/bin
ExecStart=/path/to/recruitment-agent/venv/bin/python main.py schedule --cron "0 9 * * *"
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

# 启用并启动服务
sudo systemctl enable recruitment-agent
sudo systemctl start recruitment-agent

# 查看状态
sudo systemctl status recruitment-agent

# 查看日志
sudo journalctl -u recruitment-agent -f
```

## 数据管理

### Notion数据库操作

#### 查看数据库结构
```python
from notion_integration import NotionIntegration

notion = NotionIntegration()
schema = notion.get_database_schema()
print(schema)
```

#### 批量导入数据
```python
from models import Candidate
from notion_integration import NotionIntegration

# 准备候选人数据
candidates = [
    Candidate(
        name="张三",
        title="软件工程师",
        company="阿里巴巴",
        source_platform="LinkedIn"
    ),
    # ... 更多候选人
]

# 批量创建
notion = NotionIntegration()
page_ids = notion.batch_create_candidates(candidates)
print(f"创建了 {len(page_ids)} 个页面")
```

#### 数据导出
```python
# 获取所有候选人
all_candidates = notion.get_all_candidates()

# 导出为CSV
import csv
with open('candidates.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(['姓名', '职位', '公司', '来源平台'])
    
    for candidate in all_candidates:
        props = candidate['properties']
        name = props.get('姓名', {}).get('title', [{}])[0].get('text', {}).get('content', '')
        title = props.get('职位', {}).get('rich_text', [{}])[0].get('text', {}).get('content', '')
        company = props.get('公司', {}).get('rich_text', [{}])[0].get('text', {}).get('content', '')
        platform = props.get('来源平台', {}).get('select', {}).get('name', '')
        
        writer.writerow([name, title, company, platform])
```

### 数据清理

#### 删除重复数据
```python
# 查找重复候选人
seen_names = set()
duplicates = []

for candidate in all_candidates:
    name = candidate['properties']['姓名']['title'][0]['text']['content']
    if name in seen_names:
        duplicates.append(candidate['id'])
    else:
        seen_names.add(name)

# 删除重复项
for page_id in duplicates:
    notion.delete_candidate_page(page_id)
```

#### 数据验证
```python
# 检查必填字段
incomplete = []
for candidate in all_candidates:
    props = candidate['properties']
    name = props.get('姓名', {}).get('title', [{}])[0].get('text', {}).get('content', '')
    if not name:
        incomplete.append(candidate['id'])

print(f"发现 {len(incomplete)} 个不完整的记录")
```

## 监控和维护

### 日志分析
```bash
# 查看最近的日志
tail -f recruitment_agent.log

# 搜索错误
grep "ERROR" recruitment_agent.log

# 统计成功率
grep -c "成功" recruitment_agent.log
```

### 性能监控
```python
# 查看运行统计
import json
with open('agent_statistics.json', 'r') as f:
    stats = json.load(f)

# 分析最近10次运行
recent_runs = stats[-10:]
avg_duration = sum(run['duration_seconds'] for run in recent_runs) / len(recent_runs)
avg_candidates = sum(run['total_candidates'] for run in recent_runs) / len(recent_runs)

print(f"平均运行时间: {avg_duration:.2f}秒")
print(f"平均候选人数: {avg_candidates:.1f}个")
```

### 错误处理

#### 常见错误及解决方案

**1. Notion API限制**
```
错误: Rate limit exceeded
解决: 增加REQUEST_DELAY，减少并发请求
```

**2. LinkedIn反爬检测**
```
错误: Access denied
解决: 
- 增加请求间隔
- 使用代理IP
- 考虑使用第三方服务
```

**3. Twitter API配额超限**
```
错误: Rate limit exceeded
解决: 
- 等待配额重置
- 升级API计划
- 减少搜索频率
```

**4. 网络连接问题**
```
错误: Connection timeout
解决:
- 检查网络连接
- 配置代理
- 增加超时时间
```

### 备份和恢复

#### 配置备份
```bash
# 备份配置文件
cp .env .env.backup.$(date +%Y%m%d)

# 备份整个项目
tar -czf recruitment-agent-backup-$(date +%Y%m%d).tar.gz \
  --exclude=venv \
  --exclude=__pycache__ \
  --exclude=*.log \
  .
```

#### 数据备份
```python
# 导出Notion数据
import json
from notion_integration import NotionIntegration

notion = NotionIntegration()
all_data = notion.get_all_candidates()

with open(f'notion_backup_{datetime.now().strftime("%Y%m%d")}.json', 'w') as f:
    json.dump(all_data, f, indent=2, ensure_ascii=False)
```

## 最佳实践

### 1. 搜索策略
- 使用具体的职位关键词而非泛泛的描述
- 结合地点和技能进行精准搜索
- 定期更新搜索关键词以获取新的候选人

### 2. 数据质量
- 定期检查和清理重复数据
- 验证联系信息的有效性
- 及时更新候选人状态

### 3. 合规使用
- 遵守各平台的使用条款
- 尊重候选人隐私
- 避免过度频繁的请求

### 4. 性能优化
- 合理设置请求间隔
- 使用缓存减少重复请求
- 监控API使用配额

### 5. 安全考虑
- 定期轮换API密钥
- 使用环境变量存储敏感信息
- 限制网络访问权限

## 故障排除

### 诊断工具
```bash
# 检查系统状态
python main.py status

# 运行诊断
python -c "
import sys
print(f'Python版本: {sys.version}')
print(f'工作目录: {os.getcwd()}')

try:
    import requests
    print('✅ requests可用')
except ImportError:
    print('❌ requests不可用')

# ... 检查其他依赖
"

# 测试网络连接
python -c "
import requests
try:
    r = requests.get('https://api.notion.com', timeout=5)
    print('✅ Notion API可达')
except:
    print('❌ Notion API不可达')
"
```

### 调试模式
```bash
# 启用详细日志
export LOG_LEVEL=DEBUG
python main.py run

# 单步调试
python -m pdb main.py run
```

---

更多高级用法和技巧，请参考 [API文档](API.md) 和 [开发指南](DEVELOPMENT.md)。

